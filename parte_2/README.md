# Web Scraper API con Supabase

API de scraping de productos que extrae informaci√≥n de libros y la almacena en Supabase.

## Caracter√≠sticas

- üï∑Ô∏è Web scraping robusto con manejo de errores
- üóÑÔ∏è Base de datos Supabase con PostgreSQL
- üîç Filtros avanzados por t√≠tulo, categor√≠a y precio
- üìä API REST documentada con FastAPI
- üõ°Ô∏è Seguridad con Row Level Security (RLS)
- üìà Logging detallado para monitoreo

## Configuraci√≥n

1. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configurar Supabase:**
   - Las variables de entorno se configurar√°n autom√°ticamente

3. **Ejecutar migraciones:**
   Las migraciones se aplicar√°n autom√°ticamente cuando conectes Supabase

## Uso

### Iniciar el servidor
```bash
python main.py
```

## Documentaci√≥n Visual

### 1. Interfaz de Documentaci√≥n Interactiva
La API incluye documentaci√≥n interactiva generada autom√°ticamente por FastAPI. Accede a `http://localhost:8000/docs` para probar los endpoints directamente desde el navegador.

![Documentaci√≥n Interactiva](images/docs.png)

*Interfaz de FastAPI donde puedes probar el endpoint POST /scrape y todos los dem√°s endpoints de forma interactiva.*

### 2. Esquema de Base de Datos
La estructura de la base de datos en Supabase con la tabla `products` y sus pol√≠ticas de seguridad.

![Esquema de Base de Datos](images/schema.png)

*Vista del esquema de la base de datos en Supabase mostrando la tabla products con sus columnas, √≠ndices y pol√≠ticas RLS.*

### 3. Proceso de Scraping en Consola
El scraper proporciona logging detallado durante su ejecuci√≥n para monitorear el progreso.

![Consola de Scraping](images/console.png)

*Salida de la consola mostrando el proceso de scraping en tiempo real con informaci√≥n detallada de cada p√°gina procesada.*

### 4. Datos Insertados en la Base de Datos
Los productos extra√≠dos se almacenan autom√°ticamente en Supabase.

![Tabla de Productos](images/tables.png)

*Vista de la tabla products en Supabase con los datos de libros insertados despu√©s del scraping.*

## Endpoints Disponibles

### `POST /scrape`
Ejecuta el scraper y guarda los productos en la base de datos.

**Respuesta:**
```json
{
  "message": "Scraping completado exitosamente",
  "total_scrapeado": 1000,
  "productos_nuevos_guardados": 850,
  "duplicados_omitidos": 150
}
```

### `GET /products`
Obtiene productos con filtros opcionales.

**Par√°metros de consulta:**
- `title`: Filtrar por t√≠tulo (b√∫squeda parcial)
- `category`: Filtrar por categor√≠a
- `min_price`: Precio m√≠nimo
- `max_price`: Precio m√°ximo
- `limit`: N√∫mero m√°ximo de resultados (default: 100)

**Ejemplo:**
```
GET /products?title=python&min_price=10&max_price=50&limit=20
```

**Respuesta:**
```json
{
  "productos": [
    {
      "id": 1,
      "title": "Learning Python",
      "price": 29.99,
      "category": "Books",
      "rating": 4,
      "created_at": "2025-01-27T10:00:00Z"
    }
  ],
  "cantidad": 1,
  "filtros_aplicados": {
    "title": "python",
    "min_price": 10,
    "max_price": 50,
    "limit": 20
  }
}
```

### `GET /categories`
Obtiene todas las categor√≠as disponibles.

**Respuesta:**
```json
{
  "categorias": ["Books", "Electronics", "Clothing"],
  "cantidad": 3
}
```

### `GET /docs`
Accede a la documentaci√≥n interactiva de la API donde puedes probar todos los endpoints.

### `GET /health`
Endpoint de verificaci√≥n de estado del servicio.

**Respuesta:**
```json
{
  "estado": "saludable",
  "servicio": "API de Web Scraper"
}
```

## Estructura de la Base de Datos

### Tabla `products`
- `id`: Clave primaria auto-incremental
- `title`: T√≠tulo del producto (indexado)
- `price`: Precio con precisi√≥n decimal (10,2)
- `category`: Categor√≠a del producto (indexado)
- `rating`: Calificaci√≥n de 0-5 estrellas (con validaci√≥n)
- `created_at`: Fecha de creaci√≥n (autom√°tica)
- `updated_at`: Fecha de actualizaci√≥n (autom√°tica)

### √çndices
- `idx_products_title`: √çndice en el campo t√≠tulo para b√∫squedas r√°pidas
- `idx_products_category`: √çndice en categor√≠a para filtros eficientes
- `idx_products_price`: √çndice en precio para rangos de b√∫squeda

## Caracter√≠sticas de Robustez

- **Manejo de errores**: Captura y registra errores de red y parsing
- **L√≠mites de p√°ginas**: Previene loops infinitos (m√°ximo 50 p√°ginas)
- **Delays aleatorios**: Respeta el servidor objetivo (1-3 segundos entre peticiones)
- **Timeouts**: Evita conexiones colgadas (10 segundos)
- **Validaci√≥n de datos**: Verifica la integridad antes de guardar
- **Logging detallado**: Facilita el debugging y monitoreo
- **Detecci√≥n de duplicados**: Evita insertar productos ya existentes

## Seguridad

- **Row Level Security (RLS)** habilitado en todas las tablas
- **Pol√≠ticas de acceso granulares**:
  - Lectura p√∫blica para usuarios an√≥nimos y autenticados
  - Escritura restringida al rol de servicio
- **Variables de entorno** para credenciales sensibles
- **Validaci√≥n de par√°metros** de entrada en todos los endpoints
- **Headers de User-Agent** para evitar bloqueos del scraper

## C√≥mo Probar la API

1. **Inicia el servidor:**
   ```bash
   python main.py
   ```

2. **Accede a la documentaci√≥n interactiva:**
   ```
   http://localhost:8000/docs
   ```

3. **Ejecuta el scraper:**
   - En `/docs`, busca `POST /scrape`
   - Haz clic en "Try it out" ‚Üí "Execute"

4. **Consulta los productos:**
   ```
   http://localhost:8000/products
   ```

5. **Aplica filtros:**
   ```
   http://localhost:8000/products?category=Books&min_price=10&max_price=30
   ```

## Tecnolog√≠as Utilizadas

- **FastAPI**: Framework web moderno y r√°pido
- **Supabase**: Base de datos PostgreSQL como servicio
- **BeautifulSoup4**: Parsing de HTML para web scraping
- **Requests**: Cliente HTTP para peticiones web
- **Uvicorn**: Servidor ASGI de alto rendimiento
- **Python-dotenv**: Gesti√≥n de variables de entorno